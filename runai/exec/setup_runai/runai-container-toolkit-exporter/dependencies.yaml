name: has_everything
spec:
  requirements:
  - has_cluster_prepared
  - has_runai
---
name: has_cluster_prepared
spec:
  requirements:
  #- has_entitlement
  - has_nfd
  - has_gpu_nodes
  - has_gpu_operator
---
name: has_runai
spec:
  requirements:
  - has_runai_config_file
  - doesnt_have_nvidia_dcgm_exporter
  - has_runai_operator
  - has_runai_device_plugin
---
name: has_entitlement
spec:
  tests:
  - name: has_entitlement
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      export WDM_DEPENDENCY_FILE=testing/wdm/gpu-operator.yml
      ./toolbox/wdm test has_entitlement
  install:
  - name: ensure_has_entitlement
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      export WDM_DEPENDENCY_FILE=testing/wdm/gpu-operator.yml
      ./toolbox/wdm ensure has_entitlement
---
name: has_nfd
spec:
  tests:
  - name: has_nfd
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      export WDM_DEPENDENCY_FILE=testing/wdm/gpu-operator.yml
      ./toolbox/wdm test has_nfd
  install:
  - name: ensure_has_nfd
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      export WDM_DEPENDENCY_FILE=testing/wdm/gpu-operator.yml
      ./toolbox/wdm ensure has_nfd
---
name: has_gpu_nodes
spec:
  tests:
  - name: test_has_gpu_nodes
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      export WDM_DEPENDENCY_FILE=testing/wdm/gpu-operator.yml
      ./toolbox/wdm test has_gpu_nodes
  install:
  - name: ensure_has_gpu_nodes
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      export WDM_DEPENDENCY_FILE=testing/wdm/gpu-operator.yml
      export INSTANCE_TYPE=g4dn.2xlarge
      ./toolbox/wdm ensure has_gpu_nodes
---
name: has_gpu_operator
spec:
  tests:
  - name: has_gpu_operator
    type: shell
    spec: oc get pod -l app.kubernetes.io/component=gpu-operator -A -oname | grep .
  install:
  - name: install_gpu_operator
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      ./run_toolbox.py gpu_operator deploy_from_operatorhub --channel=v1.8 --namespace openshift-operators
  - name: await_gpu_operator
    type: shell
    spec: |
      cd ${HOME}/openshift/ci-artifacts
      ./run_toolbox.py gpu_operator wait_deployment
---
name: has_runai_config_file
spec:
  tests:
  - name: has_runai_config_file_env
    type: shell
    spec: test "${RUN_AI_CONFIG:-}"

  - name: has_runai_config_file
    type: shell
    spec: file "$RUN_AI_CONFIG"
  install:
  - name: cannot_find_config_file
    type: shell
    spec: |
      echo "RUN_AI_CONFIG env var must point to the file generated by app.run.ai"
      false
---
name: has_runai_operator
spec:
  requirements:
  - has_runai_config_file
  tests:
  - name: has_runai_crd
    type: shell
    spec: oc get crd/runaijobs.run.ai -oname
  install:
  - name: install_runai
    type: shell
    spec: |
      # from https://app.run.ai/clusters <> new cluster
      oc new-project runai || true
      oc label namespace runai openshift.io/cluster-monitoring=true

      helm repo add runai https://run-ai-charts.storage.googleapis.com
      helm repo update
      helm upgrade -i runai-cluster runai/runai-cluster -n runai -f "$RUN_AI_CONFIG"
---
name: has_runai_device_plugin
spec:
  tests:
  - name: has_runai_device_plugin_configured
    type: shell
    spec: test $(oc get clusterpolicy/gpu-cluster-policy -ojsonpath={.spec.devicePlugin.repository}) == "gcr.io/run-ai-prod"
  install:
  - name: set_runai_device_plugin
    type: shell
    spec: |
      oc create clusterrolebinding --clusterrole=admin --serviceaccount=gpu-operator-resources:nvidia-device-plugin nvidia-device-plugin-crb
      oc patch clusterpolicy/gpu-cluster-policy --type='json' -p='[{"op": "replace", "path": "/spec/devicePlugin/repository", "value": "gcr.io/run-ai-prod"}]'
      oc patch clusterpolicy/gpu-cluster-policy --type='json' -p='[{"op": "replace", "path": "/spec/devicePlugin/image", "value": "nvidia-device-plugin"}]'
      oc patch clusterpolicy/gpu-cluster-policy --type='json' -p='[{"op": "replace", "path": "/spec/devicePlugin/version", "value": "1.0.11"}]'
---
name: doesnt_have_nvidia_dcgm_exporter
spec:
  tests:
  - name: doesnt_have_nvidia_dcgm_exporter
    type: shell
    spec: test -z $(oc get nodes -lnvidia.com/gpu.deploy.dcgm-exporter=true -oname)
  install:
  - name: disable_nvidia_dcgm_exporter
    type: shell
    spec: |
      for node in $(oc get nodes -lnvidia.com/gpu.deploy.dcgm-exporter=true -oname); do
        oc label $node nvidia.com/gpu.deploy.dcgm-exporter=runai --overwrite
      done
# runai-adm set node-role --runai-system-worker ip-10-0-134-32.eu-central-1.compute.internal
